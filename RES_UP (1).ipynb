{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041cfbdb-d69d-4584-a96f-e5f9642629e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90729c63-fbcc-47bd-a426-9b57a33e3420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (1.25.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ba1b45d-9027-457b-80c8-1da61ea56ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d01fb7-5ba6-4bef-962e-c0d513c02af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ™ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ´ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ¦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â § \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‡ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â � \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest â ‹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–� 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�   96 B                         \n",
      "pulling 34bb5ab01051... 100% â–•â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–�  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e4d1e7-4067-4fa0-bfbb-c52d3b8d555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF processing\n",
    "import re\n",
    "import requests\n",
    "import gradio as gr\n",
    "import time  # Measure time for performance testing\n",
    "\n",
    "# Ollama API details\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13131420-59ba-4e7b-9489-a7d00e09facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client for Ollama integration\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45faa683-9fb5-4a1c-b241-1fa405ac5f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\gad\\anaconda3\\envs\\llms\\lib\\site-packages (from pytesseract) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3e4dd1-e360-40e6-9c8e-cd790eb03208",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GAD\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\interface.py:403: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://67167cb9a1b4447f7e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://67167cb9a1b4447f7e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from model: Yes, this profile can develop high fidelity wireframes. The skills section of the resume mentions \"Wireframing\" as one of his techniques, and he has experience in developing low-fidelity wireframes during his UI/UX internship at CognoRise info tech. Additionally, he also mentions delivering interactive prototypes in his previous roles, which implies that he is familiar with creating detailed and realistic designs.\n",
      "\n",
      "However, it's worth noting that the resume does not explicitly mention \"High Fidelity Wireframing\" as a specific skill or experience. But based on the information provided, it can be inferred that this profile has the capability to develop high fidelity wireframes, given his background in wireframing and prototyping.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import gradio as gr\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "from openai import OpenAI\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "# Initialize the OpenAI client for Ollama integration\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "# Define the System message\n",
    "system_message = \"You are a highly knowledgeable assistant who can answer questions based on the user's resume. Use your understanding of the resume content to provide accurate and insightful answers.\"\n",
    "\n",
    "# Function to extract text from the uploaded PDF resume (optimized)\n",
    "def extract_text_from_pdf(pdf_file, max_pages=5):\n",
    "    \"\"\"\n",
    "    Extracts text content from the uploaded PDF file. Optimized to extract from the first few pages.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file (file): The uploaded PDF resume file.\n",
    "        max_pages (int): The number of pages to extract text from.\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF using PyMuPDF\n",
    "        doc = fitz.open(pdf_file)\n",
    "        text = \"\"\n",
    "        \n",
    "        # Limit the number of pages to extract from\n",
    "        page_count = min(doc.page_count, max_pages)\n",
    "        \n",
    "        for page_num in range(page_count):\n",
    "            page = doc.load_page(page_num)\n",
    "            text += page.get_text(\"text\")\n",
    "        \n",
    "        # Only return the text if there's something to extract\n",
    "        if text.strip():\n",
    "            return text.strip()\n",
    "        else:\n",
    "            return \"No extractable text found in the PDF.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during PDF extraction: {e}\")  # Log the error for debugging\n",
    "        return f\"Error extracting text from PDF: {e}\"\n",
    "\n",
    "# Function to answer questions based on the resume content\n",
    "def answer_resume_question(question, resume_text, history):\n",
    "    \"\"\"\n",
    "    Answers the user's question based on the resume content.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The user's question.\n",
    "        resume_text (str): The extracted text from the resume.\n",
    "        history (list): Conversation history.\n",
    "        \n",
    "    Returns:\n",
    "        str: The answer based on the resume content.\n",
    "        list: Updated conversation history.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a prompt with the resume text and user's question\n",
    "        prompt = (\n",
    "            f\"The following is a resume text:\\n\\n{resume_text}\\n\\n\"\n",
    "            f\"Answer the following question based on the resume content:\\n\\n\"\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        \n",
    "        # Make a request to the model\n",
    "        response = ollama_via_openai.completions.create(\n",
    "            model=MODEL,  # Llama model\n",
    "            prompt=prompt,\n",
    "            max_tokens=150,  # Limit the length of the answer\n",
    "            temperature=0.6,  # Moderate creativity to avoid over-generation\n",
    "        )\n",
    "        \n",
    "        # Extract the answer from the response\n",
    "        answer = response.choices[0].text.strip()\n",
    "        print(f\"Answer from model: {answer}\")  # Log the response for debugging\n",
    "        \n",
    "        # Append the question and answer to history\n",
    "        history.append({\"role\": \"user\", \"content\": question})\n",
    "        history.append({\"role\": \"assistant\", \"content\": answer})\n",
    "        \n",
    "        return answer, history\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model request: {e}\")  # Log the error for debugging\n",
    "        return f\"An error occurred: {e}\", history\n",
    "\n",
    "# Define the Gradio UI and chat function\n",
    "def chat_with_resume_qa(pdf_file, question, history):\n",
    "    \"\"\"\n",
    "    Handles the user's question and answers based on the uploaded resume.\n",
    "    \n",
    "    Args:\n",
    "        pdf_file (file): The uploaded resume PDF.\n",
    "        question (str): The user's question about the resume.\n",
    "        history (list): Conversation history.\n",
    "        \n",
    "    Returns:\n",
    "        str: The assistant's answer based on the resume.\n",
    "        list: Updated history.\n",
    "    \"\"\"\n",
    "    # Ensure history is a list (if None, initialize as an empty list)\n",
    "    history = history or []\n",
    "    \n",
    "    # Extract the text from the PDF\n",
    "    resume_text = extract_text_from_pdf(pdf_file.name)\n",
    "    \n",
    "    if \"Error\" in resume_text or \"No extractable text found\" in resume_text:\n",
    "        print(f\"PDF extraction error: {resume_text}\")  # Log extraction error\n",
    "        return resume_text, history\n",
    "    \n",
    "    # Call the function to generate an answer based on the resume\n",
    "    response, history = answer_resume_question(question, resume_text, history)\n",
    "    \n",
    "    return response, history\n",
    "\n",
    "# Initialize the Gradio interface for resume-based Q&A\n",
    "gr.Interface(\n",
    "    fn=chat_with_resume_qa,\n",
    "    inputs=[gr.File(label=\"Upload Your Resume (PDF)\", type=\"filepath\"),\n",
    "            gr.Textbox(label=\"Ask a Question about the Resume\", placeholder=\"Enter your question...\", lines=2),\n",
    "            gr.State()],\n",
    "    outputs=[gr.Textbox(label=\"Assistant's Answer\", interactive=False), gr.State()],\n",
    "    live=False,  # Disable real-time submission\n",
    "    title=\"Resume Q&A Chatbot\",\n",
    "    description=\"Upload your resume and ask questions about it. The chatbot will answer based on the resume content.\",\n",
    "    allow_flagging=\"never\"\n",
    ").launch(share = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36caab32-2aec-4888-9460-92d2a48324b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2519eeb-334c-49e1-b5cc-08e7aba150c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093938e7-ce14-467e-b906-35cb37af091a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed44df-fd0a-4fc5-96c9-fe3a19916420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
